# ğŸš€ AirFlow ETL : Data Pipeline Orchestration

Pipeline ETL complÃ¨te pour l'ingestion et la transformation de donnÃ©es avec Apache Airflow, PostgreSQL et Superset.

## ğŸ“‹ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Extract   â”‚â”€â”€â”€â–¶â”‚  Transform  â”‚â”€â”€â”€â–¶â”‚    Load     â”‚â”€â”€â”€â–¶â”‚    KPIs     â”‚
â”‚  (CSV/Excel)â”‚    â”‚  (Pandas)   â”‚    â”‚ (PostgreSQL)â”‚    â”‚ (Dashboard) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ›  Stack Technique

| Service | Port | Description |
|---------|------|-------------|
| **Airflow** | 8081 | Orchestration des workflows |
| **PostgreSQL** | 5432 | Base de donnÃ©es |
| **Superset** | 8088 | Visualisation |

## ğŸ“š Documentation

## ğŸš€ Installation Rapide

### PrÃ©requis
- Docker & Docker Compose installÃ©s
- 4GB RAM minimum

### Lancement

```bash
# 1. Cloner le projet
cd airflow_etl

# 2. CrÃ©er les dossiers nÃ©cessaires
mkdir -p logs plugins

# 3. Initialiser Airflow
docker-compose up airflow-init

# 4. Lancer tous les services
docker-compose up -d

# 5. VÃ©rifier les services
docker-compose ps
```

## ğŸ” AccÃ¨s aux Services

| Service | URL | Login | Password |
|---------|-----|-------|----------|
| Airflow | http://localhost:8080 | admin | admin |
| Superset | http://localhost:8088 | admin | admin |
| PostgreSQL | localhost:5432 | airflow | airflow |

## ğŸ“Š Configuration de la Connexion PostgreSQL dans Airflow

1. Aller dans **Admin > Connections**
2. CrÃ©er une nouvelle connexion:
   - **Connection Id**: `postgres_etl`
   - **Connection Type**: Postgres
   - **Host**: `postgres`
   - **Schema**: `etl_data`
   - **Login**: `airflow`
   - **Password**: `airflow`
   - **Port**: `5432`

## ğŸ“ˆ Configuration Superset

1. Aller dans **Settings > Database Connections**
2. Ajouter PostgreSQL:
   ```
   postgresql://airflow:airflow@postgres:5432/etl_data
   ```
3. CrÃ©er des datasets sur les tables:
   - `sales_transformed`
   - `sales_kpis`

## ğŸ“ Structure du Projet

```
airflow_etl/
â”œâ”€â”€ docker-compose.yml      # Configuration Docker
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ etl_sales_pipeline.py   # DAG ETL principal
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sales_data.csv      # DonnÃ©es source
â”œâ”€â”€ init-db/
â”‚   â””â”€â”€ 01-init.sql         # Script init PostgreSQL
â”œâ”€â”€ logs/                   # Logs Airflow
â””â”€â”€ plugins/                # Plugins Airflow
```

## ğŸ”„ Pipeline ETL

Le DAG `etl_sales_pipeline` effectue:

1. **Extract**: Lecture des fichiers CSV
2. **Transform**: Nettoyage avec Pandas
   - Suppression des valeurs nulles
   - Conversion des types
   - Calcul du montant total
3. **Load**: Insertion dans PostgreSQL
4. **KPIs**: Calcul des indicateurs

## ğŸ§ª Tester le DAG

```bash
# Tester une tÃ¢che spÃ©cifique
docker-compose exec airflow-scheduler airflow tasks test etl_sales_pipeline extract_data 2024-01-01
```

## ğŸ›‘ ArrÃªter les Services

```bash
docker-compose down

# Supprimer les volumes (reset complet)
docker-compose down -v

## ğŸ“¤ Pousser ce projet sur GitHub

1. CrÃ©ez un repository sur GitHub (vide) et copiez l'URL (SSH ou HTTPS).
2. ExÃ©cutez le script d'aide (ou les commandes manuelles ci-dessous) depuis la racine du projet:

```bash
# Avec le script helper (recommandÃ©)
chmod +x scripts/git_init_and_push.sh
./scripts/git_init_and_push.sh git@github.com:USERNAME/REPO.git main

# OU manuellement:
git init
git add .
git commit -m "chore: initial import"
git branch -M main
git remote add origin <REMOTE_URL>
git push -u origin main
```

Remarques:
- VÃ©rifiez le fichier `.gitignore` avant de pousser pour vous assurer qu'aucun secret n'est inclus.
- Le dÃ©pÃ´t contient un workflow GitHub Actions `/.github/workflows/ci.yml` qui exÃ©cutera des checks de lint sur chaque push/PR.
