# ğŸš€ AirFlow ETL : Data Pipeline Orchestration

Une solution complÃ¨te d'orchestration de pipelines ETL (Extract-Transform-Load) utilisant **Apache Airflow**, **PostgreSQL** et **Superset** pour l'ingestion, la transformation et la visualisation des donnÃ©es en production.

## ğŸ“‹ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Extract       â”‚â”€â”€â”€â–¶â”‚   Transform      â”‚â”€â”€â”€â–¶â”‚     Load        â”‚â”€â”€â”€â–¶â”‚    KPIs &    â”‚
â”‚ (CSV/Excel)     â”‚    â”‚ (Pandas/Python)  â”‚    â”‚ (PostgreSQL DB) â”‚    â”‚  Dashboard   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“                      â†“                        â†“                    â†“
    Raw Data             Cleaned Data            Transformed Data      Visualization
```

## ğŸ›  Stack Technique

| Service | Port | Description | Status |
|---------|------|-------------|--------|
| **Airflow UI** | 8080 | Orchestration et monitoring des workflows | âœ… |
| **PostgreSQL** | 5432 | Base de donnÃ©es relationnelle | âœ… |
| **Superset** | 8088 | Visualisation et dashboards | âœ… |

## ğŸ“š Documentation

- ğŸ“– [Architecture DÃ©taillÃ©e](docs/ARCHITECTURE.md) - Vue d'ensemble du systÃ¨me
- ğŸ”„ [DAGs et Pipelines](docs/DAGS.md) - Description des workflows ETL

## ğŸš€ Installation Rapide

### PrÃ©requis
- **Docker** & **Docker Compose** (v20.10+)
- **4GB RAM** minimum
- **5GB** espace disque disponible
- AccÃ¨s Internet pour tÃ©lÃ©charger les images

### Ã‰tapes d'Installation

```bash
# 1ï¸âƒ£ Cloner le projet
git clone <votre-url-repo>
cd airflow-etl-project

# 2ï¸âƒ£ CrÃ©er les dossiers nÃ©cessaires
mkdir -p logs plugins

# 3ï¸âƒ£ Configurer les variables d'environnement
cp env.example .env
# Ã‰diter .env si nÃ©cessaire

# 4ï¸âƒ£ Initialiser Airflow (premiÃ¨re fois seulement)
docker-compose up airflow-init

# 5ï¸âƒ£ Lancer tous les services
docker-compose up -d

# 6ï¸âƒ£ VÃ©rifier l'Ã©tat des services
docker-compose ps
```

**Temps estimÃ© :** 3-5 minutes aprÃ¨s le tÃ©lÃ©chargement des images Docker

## ğŸ” AccÃ¨s aux Services

| Service | URL | Identifiant | Mot de passe | Notes |
|---------|-----|-------------|-------------|-------|
| **Airflow UI** | http://localhost:8080 | admin | admin | Orchestration des DAGs |
| **Superset** | http://localhost:8088 | admin | admin | Dashboards & Visualisation |
| **PostgreSQL** | localhost:5432 | airflow | airflow | Client: psql, DBeaver, etc. |

> âš ï¸ **Production**: Changez les mots de passe par dÃ©faut dans `.env`

## ğŸ“Š Configuration de PostgreSQL dans Airflow

Airflow se configure automatiquement avec PostgreSQL lors du dÃ©marrage. Pour configurer manuellement une nouvelle connexion:

### Via l'Interface Web

1. AccÃ©dez Ã  **Admin > Connections** dans Airflow UI
2. Cliquez sur **"+ Connexion"** (ou "Create")
3. Remplissez les dÃ©tails suivants:
   - **Connection Id**: `postgres_etl`
   - **Connection Type**: `Postgres`
   - **Host**: `postgres` (nom du service Docker)
   - **Database**: `etl_data`
   - **Login**: `airflow`
   - **Password**: `airflow`
   - **Port**: `5432`
4. Cliquez sur **Save**

### Via Script

```bash
# Utiliser le script fourni (recommandÃ©)
python init-airflow-connection.py
```

## ğŸ“ˆ Configuration Superset

### Connexion Ã  la Base de DonnÃ©es

1. AccÃ©dez Ã  **Settings > Database Connections** (icÃ´ne engrenage)
2. Cliquez sur **"+ New"** ou **"Create"**
3. SÃ©lectionnez **PostgreSQL** comme type
4. Entrez l'URI de connexion:
   ```
   postgresql://airflow:airflow@postgres:5432/etl_data
   ```
5. Testez la connexion et sauvegardez

### CrÃ©ation de Datasets et Dashboards

Les tables importantes Ã  visualiser:
- `sales_transformed` - DonnÃ©es de ventes nettoyÃ©es
- `sales_kpis` - Indicateurs clÃ©s calculÃ©s
- `products` - Catalogue de produits

Utilisez l'interface Superset pour crÃ©er des datasets et des visualisations personnalisÃ©es.

> ğŸ“– Voir la [configuration complÃ¨te de Superset](superset/superset_config.py)

## ğŸ“ Structure du Projet

```
airflow-etl-project/
â”œâ”€â”€ ğŸ“„ docker-compose.yml           # Configuration Docker (services)
â”œâ”€â”€ ğŸ“„ .env.example                 # Variables d'environnement (template)
â”œâ”€â”€ ğŸ“„ README.md                    # Ce fichier
â”‚
â”œâ”€â”€ ğŸ“‚ dags/                        # DAGs Airflow
â”‚   â”œâ”€â”€ products_pipeline.py        # Pipeline pour les produits
â”‚   â””â”€â”€ sales_pipeline.py           # Pipeline pour les ventes
â”‚
â”œâ”€â”€ ğŸ“‚ data/                        # DonnÃ©es sources
â”‚   â”œâ”€â”€ products.csv
â”‚   â””â”€â”€ sales.csv
â”‚
â”œâ”€â”€ ğŸ“‚ docs/                        # Documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md             # Architecture systÃ¨me
â”‚   â””â”€â”€ DAGS.md                     # Description des DAGs
â”‚
â”œâ”€â”€ ğŸ“‚ init-db/                     # Scripts d'initialisation DB
â”‚   â”œâ”€â”€ init.sql
â”‚   â””â”€â”€ schema.sql
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/                     # Utilitaires
â”‚   â””â”€â”€ git_init_and_push.sh        # Initialiser GitHub
â”‚
â”œâ”€â”€ ğŸ“‚ superset/                    # Configuration Superset
â”‚   â””â”€â”€ superset_config.py
â”‚
â”œâ”€â”€ ğŸ“‚ logs/                        # Logs Airflow (gÃ©nÃ©rÃ©)
â””â”€â”€ ğŸ“‚ plugins/                     # Plugins Airflow (gÃ©nÃ©rÃ©)
```

## ğŸ”„ Pipelines ETL

### Sales Pipeline (`sales_pipeline.py`)
Traite les donnÃ©es de ventes:

1. **Extract** ğŸ“¥
   - Lecture du fichier `data/sales.csv`
   - Validation du format

2. **Transform** ğŸ”§
   - Suppression des valeurs nulles
   - Conversion des types de donnÃ©es
   - Calcul du montant total (quantity Ã— unit_price)
   - Ajout de timestamps

3. **Load** ğŸ“¤
   - Insertion dans `sales_transformed` (PostgreSQL)
   - Calcul des KPIs
   - Stockage dans `sales_kpis`

### Products Pipeline (`products_pipeline.py`)
Traite le catalogue de produits:

1. Extraction depuis `data/products.csv`
2. Nettoyage et normalisation
3. Chargement dans la table `products`

## ğŸ§ª Tester les DAGs

### Lancer un DAG complet
```bash
# DÃ©clencher manuellement le DAG sales_pipeline
docker-compose exec airflow-scheduler airflow dags trigger sales_pipeline
```

### Tester une tÃ¢che spÃ©cifique
```bash
# Tester l'extraction de donnÃ©es
docker-compose exec airflow-scheduler airflow tasks test sales_pipeline extract_sales 2024-01-01
```

### Voir les logs
```bash
# Consulter les logs d'une tÃ¢che
docker-compose exec airflow-scheduler airflow tasks logs sales_pipeline extract_sales 2024-01-01
```

### VÃ©rifier l'Ã©tat des DAGs
```bash
# Lister tous les DAGs
docker-compose exec airflow-scheduler airflow dags list

# Ã‰tat du DAG
docker-compose exec airflow-scheduler airflow dags state sales_pipeline
```

## ğŸ›‘ ArrÃªter et Nettoyer

### ArrÃªter les Services
```bash
# ArrÃªter sans supprimer les volumes (donnÃ©es persistantes)
docker-compose stop

# ArrÃªter et supprimer les conteneurs
docker-compose down

# Reset complet (attention: supprime toutes les donnÃ©es!)
docker-compose down -v
```

### Nettoyer les Logs
```bash
# Supprimer les anciens logs Airflow
rm -rf logs/*
```

## ğŸ“¤ DÃ©ployer sur GitHub

### Initialiser et Pousser le Repository

1. **CrÃ©ez un nouveau repository** sur [GitHub](https://github.com/new) (vide, sans README)
2. **Copiez l'URL** (SSH recommandÃ© si vous avez SSH keys configurÃ©es)

3. **ExÃ©cutez le script automatisÃ©** (recommandÃ©):
```bash
chmod +x scripts/git_init_and_push.sh
./scripts/git_init_and_push.sh git@github.com:VOTRE_USERNAME/VOTRE_REPO.git main
```

4. **OU procÃ©dez manuellement:**
```bash
git init
git add .
git commit -m "chore: initial import - Airflow ETL project"
git branch -M main
git remote add origin git@github.com:VOTRE_USERNAME/VOTRE_REPO.git
git push -u origin main
```

### Points Importants

âš ï¸ **Avant de pousser**:
- VÃ©rifiez `.gitignore` pour Ã©viter les secrets (credentials, tokens)
- Les dossiers `logs/`, `plugins/` et volumes Docker ne doivent pas Ãªtre versionÃ©s
- Utilisez des variables d'environnement pour les donnÃ©es sensibles

âœ… **AprÃ¨s le push**:
- Le workflow CI/CD `/.github/workflows/ci.yml` s'exÃ©cutera automatiquement
- Les checks de lint seront appliquÃ©s Ã  chaque commit

## â“ DÃ©pannage

### Les services ne dÃ©marrent pas
```bash
# VÃ©rifier l'Ã©tat des conteneurs
docker-compose ps

# Voir les logs
docker-compose logs -f

# RedÃ©marrer les services
docker-compose restart
```

### PostgreSQL inaccessible
```bash
# VÃ©rifier la connexion PostgreSQL
docker-compose exec postgres psql -U airflow -d etl_data -c "SELECT 1;"
```

### RÃ©initialiser Airflow
```bash
docker-compose down -v
docker-compose up airflow-init
docker-compose up -d
```

## ğŸ¤ Contribution

Les contributions sont bienvenues! Pour contribuer:

1. Fork le repository
2. CrÃ©ez une branche (`git checkout -b feature/amÃ©lioration`)
3. Commitez vos changements (`git commit -m 'Add: nouvelle fonctionnalitÃ©'`)
4. Poussez vers la branche (`git push origin feature/amÃ©lioration`)
5. Ouvrez une Pull Request

## ğŸ“ Licence

Ce projet est sous licence MIT. Consultez le fichier `LICENSE` pour plus de dÃ©tails.

## ğŸ“ Support

Pour toute question ou problÃ¨me:
- ğŸ“– Consultez la [documentation technique](docs/ARCHITECTURE.md)
- ğŸ› Ouvrez une [issue GitHub](https://github.com/djeddiouael/airflow-etl-project/issues)
- ğŸ’¬ Visitez la section [Discussions](https://github.com/djeddiouael/airflow-etl-project/discussions)
