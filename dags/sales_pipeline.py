"""
DAG ETL Pipeline pour les donnÃ©es de ventes
Extract -> Transform -> Load
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
import pandas as pd
import os

# Configuration par dÃ©faut du DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Chemin des donnÃ©es
DATA_PATH = '/opt/airflow/data'


def extract_data(**kwargs):
    """
    Extract: Lire les fichiers CSV depuis le dossier data
    """
    csv_file = os.path.join(DATA_PATH, 'sales.csv')
    
    if os.path.exists(csv_file):
        df = pd.read_csv(csv_file)
        # Ajouter une colonne id si elle n'existe pas
        if 'id' not in df.columns:
            df.insert(0, 'id', range(1, len(df) + 1))
        print(f"âœ… Extraction rÃ©ussie: {len(df)} lignes lues")
        # Sauvegarder en JSON pour passer entre les tÃ¢ches
        df.to_json(os.path.join(DATA_PATH, 'extracted_data.json'), orient='records')
        return True
    else:
        print(f"âŒ Fichier non trouvÃ©: {csv_file}")
        raise FileNotFoundError(f"Fichier {csv_file} non trouvÃ©")


def transform_data(**kwargs):
    """
    Transform: Nettoyer et transformer les donnÃ©es avec Pandas
    """
    json_file = os.path.join(DATA_PATH, 'extracted_data.json')
    df = pd.read_json(json_file)
    
    print(f"ğŸ“Š DonnÃ©es avant transformation: {len(df)} lignes")
    
    # 1. Supprimer les valeurs nulles
    df = df.dropna()
    
    # 2. Convertir la colonne date
    df['date'] = pd.to_datetime(df['date'])
    
    # 3. Calculer le montant total
    df['total_amount'] = df['quantity'] * df['unit_price']
    
    # 4. Nettoyer les noms de produits (majuscules)
    df['product'] = df['product'].str.strip().str.title()
    df['category'] = df['category'].str.strip().str.title()
    df['region'] = df['region'].str.strip().str.title()
    
    # 5. Filtrer les valeurs nÃ©gatives
    df = df[df['quantity'] > 0]
    df = df[df['unit_price'] > 0]
    
    print(f"âœ… DonnÃ©es aprÃ¨s transformation: {len(df)} lignes")
    
    # Sauvegarder les donnÃ©es transformÃ©es
    df.to_json(os.path.join(DATA_PATH, 'transformed_data.json'), orient='records', date_format='iso')
    
    return True


def load_data(**kwargs):
    """
    Load: InsÃ©rer les donnÃ©es transformÃ©es dans PostgreSQL
    """
    json_file = os.path.join(DATA_PATH, 'transformed_data.json')
    df = pd.read_json(json_file)
    
    # Convertir et formater la date
    if 'date' in df.columns:
        df['date'] = pd.to_datetime(df['date']).dt.date
    
    # SÃ©lectionner les colonnes requises
    cols_to_use = ['date', 'product', 'category', 'quantity', 'unit_price', 'total_amount', 'region']
    df_insert = df[[col for col in cols_to_use if col in df.columns]].copy()
    
    # Connexion Ã  PostgreSQL
    hook = PostgresHook(postgres_conn_id='postgres_etl')
    engine = hook.get_sqlalchemy_engine()
    
    # InsÃ©rer les donnÃ©es dans la table
    df_insert.to_sql(
        'sales_transformed',
        engine,
        schema='public',
        if_exists='append',
        index=False,
        method='multi'
    )
    
    print(f"âœ… Chargement rÃ©ussi: {len(df_insert)} lignes insÃ©rÃ©es dans PostgreSQL")
    return True


def calculate_kpis(**kwargs):
    """
    Calculer les KPIs et les stocker dans la table sales_kpis
    """
    json_file = os.path.join(DATA_PATH, 'transformed_data.json')
    df = pd.read_json(json_file)
    
    if len(df) == 0:
        print("âš ï¸  Aucune donnÃ©e pour calculer les KPIs")
        return True
    
    # Calcul des KPIs
    total_sales = float(df['total_amount'].sum())
    total_quantity = int(df['quantity'].sum())
    avg_order_value = float(df['total_amount'].mean())
    top_product = df.groupby('product')['total_amount'].sum().idxmax() if len(df) > 0 else 'N/A'
    top_region = df.groupby('region')['total_amount'].sum().idxmax() if len(df) > 0 else 'N/A'
    
    kpi_data = {
        'calculation_date': datetime.now().date(),
        'total_sales': total_sales,
        'total_quantity': total_quantity,
        'avg_order_value': avg_order_value,
        'top_product': top_product,
        'top_region': top_region
    }
    
    kpi_df = pd.DataFrame([kpi_data])
    
    # Connexion Ã  PostgreSQL
    hook = PostgresHook(postgres_conn_id='postgres_etl')
    engine = hook.get_sqlalchemy_engine()
    
    kpi_df.to_sql(
        'sales_kpis',
        engine,
        schema='public',
        if_exists='append',
        index=False
    )
    
    print(f"âœ… KPIs calculÃ©s et sauvegardÃ©s:")
    print(f"   - Total Sales: {total_sales:.2f}")
    print(f"   - Total Quantity: {total_quantity}")
    print(f"   - Avg Order Value: {avg_order_value:.2f}")
    print(f"   - Top Product: {top_product}")
    print(f"   - Top Region: {top_region}")
    
    return True


# DÃ©finition du DAG
with DAG(
    'sales_pipeline',
    default_args=default_args,
    description='Pipeline ETL pour les donnÃ©es de ventes',
    schedule_interval='@daily',
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['etl', 'sales'],
) as dag:
    
    # TÃ¢che 1: Extraction
    extract_task = PythonOperator(
        task_id='extract_data',
        python_callable=extract_data,
    )
    
    # TÃ¢che 2: Transformation
    transform_task = PythonOperator(
        task_id='transform_data',
        python_callable=transform_data,
    )
    
    # TÃ¢che 3: Chargement
    load_task = PythonOperator(
        task_id='load_data',
        python_callable=load_data,
    )
    
    # TÃ¢che 4: Calcul des KPIs
    kpi_task = PythonOperator(
        task_id='calculate_kpis',
        python_callable=calculate_kpis,
    )
    
    # DÃ©finir l'ordre d'exÃ©cution
    extract_task >> transform_task >> load_task >> kpi_task

